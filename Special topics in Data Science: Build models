#########################Build_models.py#########################


import pandas as pd
import numpy as np
from imblearn.combine import SMOTETomek, SMOTEENN  # 综合采样
from imblearn.over_sampling import RandomOverSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import cohen_kappa_score, make_scorer, f1_score, balanced_accuracy_score
from Preprocessing import loadData, scaleNumericFeatures
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier  # 决策树
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from keras import models, layers
from keras.utils import to_categorical
from datetime import datetime
import xgboost as xgb
import lightgbm as lgb
import catboost as cb


def runModel(working_dir, model_type, scale_data=False, select_features=False):
    X_data, y_data, X_test = loadData(working_dir)
    # X_data, y_data = X_data.iloc[-5000:, :], y_data.iloc[-5000:]
    if scale_data:
        X_data = scaleNumericFeatures(X_data)
        X_test = scaleNumericFeatures(X_test)
    if select_features:
        X_data = X_data.drop(columns=['X17', 'X20', 'B12', 'E3', 'I6', 'I15', 'I17', 'I18', 'I19'])
        X_test = X_test.drop(columns=['X17', 'X20', 'B12', 'E3', 'I6', 'I15', 'I17', 'I18', 'I19'])

    le_y = LabelEncoder()
    y_data = le_y.fit_transform(y_data.values.flatten())
    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=2021)

    # smote_tomek = SMOTETomek(random_state=2020)
    # X_train, y_train = smote_tomek.fit_resample(X_train, y_train)  # 综合采样，没有什么效果
    smote_teenn = RandomOverSampler(random_state=2020)
    X_train, y_train = smote_teenn.fit_resample(X_train, y_train)
    print(f'label -1:{round(sum(y_train == 0) / y_train.shape[0] * 100, 1)}%, '
          f'label 0:{round(sum(y_train == 1) / y_train.shape[0] * 100, 1)}%, '
          f'label 1:{round(sum(y_train == 2) / y_train.shape[0] * 100, 1)}%  ')

    model = buildModel(X_train, X_val, y_train, y_val, model_type, lasso=select_features)

    if model_type == 'XGBoost':
        y_pred = le_y.inverse_transform(model.predict(xgb.DMatrix(X_test)).astype(np.int))
    elif model_type == 'FullyConnected':
        y_pred_onehot = model.predict(X_test)
        y_pred = le_y.inverse_transform(np.array([np.argmax(item) for item in y_pred_onehot]).astype(np.int))
    else:
        y_pred = le_y.inverse_transform(model.predict(X_test).astype(np.int))
    cust_no = pd.read_csv(r'F:\数据科学专题\competition\test\x_test\cust_avli_test\cust_avli_Q1.csv')
    pd.concat([cust_no, pd.Series(y_pred, name='label')], axis=1).to_csv(
        ''.join([r'C:\Users\DengTao\Desktop\result_', model_type, '.csv']),
        header=True, index=False)


def customizeScore(y_pred, y_true):
    Kappa = cohen_kappa_score(y_pred, y_true)
    return float(Kappa)


mape = make_scorer(customizeScore, greater_is_better=True)


def calculate_time(start, end):  # 计算时间（秒）
    pass_time = end - start
    return pass_time.seconds + pass_time.microseconds / 1e6


def add_report(model_type, performance_report, y_val, y_pred, start, end, lasso=False):
    if lasso:
        performance_report.loc['lasso-' + model_type, 'kappa'] = cohen_kappa_score(y_pred, y_val)
        performance_report.loc['lasso-' + model_type, 'micro-F1'] = f1_score(y_val, y_pred, average='micro')
        performance_report.loc['lasso-' + model_type, 'macro-F1'] = f1_score(y_val, y_pred, average='macro')
        performance_report.loc['lasso-' + model_type, 'balanced_accuracy'] = balanced_accuracy_score(y_val, y_pred)
        performance_report.loc['lasso-' + model_type, 'time'] = calculate_time(start, end)
    else:
        performance_report.loc[model_type, 'kappa'] = cohen_kappa_score(y_pred, y_val)
        performance_report.loc[model_type, 'micro-F1'] = f1_score(y_val, y_pred, average='micro')
        performance_report.loc[model_type, 'macro-F1'] = f1_score(y_val, y_pred, average='macro')
        performance_report.loc[model_type, 'balanced_accuracy'] = balanced_accuracy_score(y_val, y_pred)
        performance_report.loc[model_type, 'time'] = calculate_time(start, end)
    return None


# 性能记录
model_type = ['LogisticRegression', 'CARTTree', 'AdaBoost', 'RandomForest', 'FullyConnected', 'XGBoost',
              'LightGBM', 'CatBoost']
total_type = model_type + ['lasso-' + Type for Type in model_type]
metrics = ['kappa', 'micro-F1', 'macro-F1', 'balanced_accuracy', 'time']
performance_report = pd.DataFrame(np.zeros((len(total_type), len(metrics))), index=total_type, columns=metrics)


def buildModel(X_train, X_val, y_train, y_val, model_type, lasso=False):
    global performance_report
    print(f'*****************{model_type}*****************')
    start = datetime.now()
    if model_type == 'XGBoost':
        params = {
            'booster': 'gbtree',
            'objective': 'multi:softmax',
            'num_class': 3,
            'learning_rate': 0.07,
            'gamma': 0.1,  # 在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值
            'max_depth': 6,  # 表示树的最大深度。也是用来避免过拟合的。当它的值越大时，模型会学到更具体更局部的样本
            'lambda': 2,  # 权重的L2正则化项
            'subsample': 0.7,  # 控制对于每棵树，随机采样的比例
            'colsample_bytree': 0.7,  # 用来控制每棵随机采样的列数的占比(每一列是一个特征)
            'min_child_weight': 3,  # 表示最小叶子节点样本权重的和。可用于避免过拟合
            'seed': 1,  # 随机数的种子 设置它可以复现随机数据的结果
            'nthread': 4,  # 用来进行多线程控制，应当输入系统的核数
            # 'eval_metric': 'auc'
        }
        dtrain = xgb.DMatrix(X_train, y_train)
        model = xgb.train(params, dtrain, 500)
        dval = xgb.DMatrix(X_val)
        y_pred = model.predict(dval)
    elif model_type == 'RandomForest':
        model = RandomForestClassifier()
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
    elif model_type == 'LightGBM':
        parameters = {
            'reg_lambda': [0.03, 0.05, 0.07],
            'learning_rate': [0.07, 0.09, 0.11],
            'n_estimators': [600, 900, 1200, 1500],
            # 'num_leaves': [20, 30, 40, 50],
            # 'max_depth': [8, 10],
            # 'min_child_weight': [0.001, 0.002],
            # 'min_child_samples': [18, 19, 20, 21, 22],
        }
        lgbm = lgb.LGBMClassifier(boosting_type='gbdt',
                                  metrics='multi_logloss',
                                  silent=0,
                                  application='multiclass',
                                  random_state=1)
        gsearch = GridSearchCV(lgbm, param_grid=parameters, scoring=mape, cv=3)
        # model.fit(X_train, y_train, eval_metric=lambda y_true, y_pred: [customizeScore(y_pred, y_true)], verbose=10)
        gsearch.fit(X_train, y_train)
        model = gsearch.best_estimator_
        print('最优超参数组合：', gsearch.best_params_)
        print('交叉验证最佳kappa值：', gsearch.best_score_)
        y_pred = model.predict(X_val)
    elif model_type == 'CatBoost':
        model = cb.CatBoostClassifier(max_depth=6, reg_lambda=2, colsample_bylevel=0.7, subsample=0.7,
                                      num_trees=2000, random_seed=1, learning_rate=0.1, task_type='CPU',
                                      verbose=100, bootstrap_type='Bernoulli')
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
    elif model_type == 'FullyConnected':
        print(X_train.shape, y_train.shape)
        # X_train, X_val = X_train.iloc[:, :30], X_val.iloc[:, :30]
        model = models.Sequential()
        model.add(layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)))
        model.add(layers.Dense(3, activation='softmax'))
        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
        y_train = to_categorical(y_train)
        model.fit(X_train, y_train, epochs=100, batch_size=128)
        y_pred_onehot = model.predict(X_val)
        y_pred = np.array([np.argmax(item) for item in y_pred_onehot])
    elif model_type == 'LogisticRegression':
        model = LogisticRegression(max_iter=1000)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
    elif model_type == 'CARTTree':
        model = DecisionTreeClassifier(max_depth=4, min_samples_split=20, min_samples_leaf=5)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
    elif model_type == 'SVM':
        model = SVC(kernel='linear', degree=2, gamma=1, coef0=0)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
    elif model_type == 'AdaBoost':
        model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),
                                   algorithm="SAMME", n_estimators=200, learning_rate=0.8)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
    end = datetime.now()
    if lasso:
        add_report(model_type, performance_report, y_val, y_pred, start, end, True)
    else:
        add_report(model_type, performance_report, y_val, y_pred, start, end)
    print('验证集kappa值：', cohen_kappa_score(y_pred, y_val))
    print('验证集accuracy:', balanced_accuracy_score(y_pred, y_val))
    return model


if __name__ == '__main__':
    # for select in [True, False]:
    #     for model in model_type:
    #         if model_type == 'FullyConnected':
    #             runModel(r'F:\数据科学专题\competition', 'FullyConnected', scale_data=True, select_features=select)
    #         else:
    #             runModel(r'F:\数据科学专题\competition', model, select_features=select)
    runModel(r'F:\数据科学专题\competition', 'FullyConnected', scale_data=True)
    runModel(r'F:\数据科学专题\competition', 'FullyConnected', scale_data=True, select_features=True)

    print(performance_report)
    performance_report.to_csv(r'F:\数据科学专题\competition\performance_report.csv')
